{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import floresta\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(X, Y):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the first sequence list\n",
    "    that counts the number of occurrences of the corresponding value from the\n",
    "    second sequences list.\n",
    "    \"\"\"\n",
    "    words = [ii for i in X for ii in i if type(i) != str]\n",
    "    tags = [ii for i in Y for ii in i if type(i) != str]\n",
    "\n",
    "    pair_count = {tag:{} for tag in set(tags)}\n",
    "    \n",
    "    for tag, word in zip(tags, words):\n",
    "        pair_count[tag][word] = pair_count[tag].get(word, 0) + 1\n",
    "    \n",
    "    return pair_count\n",
    "\n",
    "\n",
    "def unigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequence list that\n",
    "    counts the number of occurrences of the value in the sequences list. The sequences\n",
    "    collection should be a 2-dimensional array.\n",
    "    \"\"\"\n",
    "    tags = [ii for i in sequences for ii in i if type(i) != str]\n",
    "    \n",
    "    unigram_counts = {tag:tags.count(tag) for tag in set(tags)}\n",
    "    \n",
    "    return unigram_counts\n",
    "\n",
    "\n",
    "def bigram_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique PAIR of values in the input sequences\n",
    "    list that counts the number of occurrences of pair in the sequences list. The input\n",
    "    should be a 2-dimensional array.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for tag1, tag2 in zip(sequence[:-1], sequence[1:]):\n",
    "            bigram_counts[(tag1, tag2)] += 1\n",
    "    \n",
    "    return bigram_counts\n",
    "\n",
    "\n",
    "def starting_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the beginning of\n",
    "    a sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    init_tags = [sentence[0] for sentence in sequences]\n",
    "    starting_counts = Counter(init_tags)\n",
    "    \n",
    "    return starting_counts\n",
    "\n",
    "\n",
    "def ending_counts(sequences):\n",
    "    \"\"\"Return a dictionary keyed to each unique value in the input sequences list\n",
    "    that counts the number of occurrences where that value is at the end of\n",
    "    a sequence.\n",
    "    \"\"\"\n",
    "    end_tags = [sentence[-1] for sentence in sequences]\n",
    "    ending_counts = Counter(end_tags)\n",
    "    \n",
    "    return ending_counts\n",
    "\n",
    "\n",
    "# For accuracy testing\n",
    "\n",
    "def my_replace_unknown(sequence, training_vocab):\n",
    "    \"\"\"Return a copy of the input sequence where each unknown word is replaced\n",
    "    by the literal string value 'nan'. Pomegranate will ignore these values\n",
    "    during computation.\n",
    "    \"\"\"\n",
    "    return [w if w in training_vocab else 'nan' for w in sequence]\n",
    "\n",
    "\n",
    "def my_simplify_decoding(X, model, training_vocab):\n",
    "    \"\"\"X should be a 1-D sequence of observations for the model to predict\"\"\"\n",
    "    _, state_path = model.viterbi(my_replace_unknown(X, training_vocab))\n",
    "    return [state[1].name for state in state_path[1:-1]]  # do not show the start/end state predictions\n",
    "\n",
    "\n",
    "def my_accuracy(X, Y, model, training_vocab):\n",
    "    \"\"\"Calculate the prediction accuracy by using the model to decode each sequence\n",
    "    in the input X and comparing the prediction with the true labels in Y.\n",
    "    \"\"\"\n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error. Any exception counts the full sentence as an error.\n",
    "        try:\n",
    "            most_likely_tags = my_simplify_decoding(observations, model, training_vocab)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus\n",
    "\n",
    "corpus = floresta.tagged_sents()\n",
    "\n",
    "\n",
    "# Separate words and tags in corpus\n",
    "\n",
    "words = [i[0] for i in [list(zip(*c)) for c in corpus]]\n",
    "tags = [i[1] for i in [list(zip(*c)) for c in corpus]]\n",
    "\n",
    "assert len(words) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(words, tags, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary used for training\n",
    "\n",
    "train_words = list(set([word for words in X_train for word in words]))\n",
    "train_tags = list(set([word for words in y_train for word in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions calls\n",
    "\n",
    "emission_counts = pair_counts(X_train, y_train)\n",
    "tag_unigrams = unigram_counts(y_train)\n",
    "tag_bigrams = bigram_counts(y_train)\n",
    "tag_starts = starting_counts(y_train)\n",
    "tag_ends = ending_counts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "model = HiddenMarkovModel(name=\"floresta-hmm-tagger\")\n",
    "\n",
    "\n",
    "# Create states with emission probability distributions P(word | tag) and add to the model\n",
    "\n",
    "states = {}\n",
    "\n",
    "for tags, words in emission_counts.items():\n",
    "    n = tag_unigrams[tags]\n",
    "    prob = {word:count/n for word, count in words.items()}\n",
    "    emissions = DiscreteDistribution(prob)\n",
    "    state = State(emissions, name=tags)\n",
    "    states[tags] = state\n",
    "    model.add_states(state)\n",
    "    \n",
    "\n",
    "    \n",
    "# Add edges between states for the observed transition frequencies P(tag_i | tag_i-1)\n",
    "\n",
    "for tags, counts in tag_starts.items():\n",
    "    model.add_transition(model.start, states[tags], counts/sum(tag_starts.values()))\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams.items():\n",
    "    model.add_transition(states[tag1], states[tag2], counts/tag_unigrams[tag1])\n",
    "\n",
    "for tags, counts in tag_ends.items():\n",
    "    model.add_transition(states[tags], model.end, counts/tag_unigrams[tags])\n",
    "    \n",
    "\n",
    "# Laplace smoothing:\n",
    "\n",
    "tag_bigrams_test = bigram_counts(y_test)\n",
    "\n",
    "for (tag1, tag2), counts in tag_bigrams_test.items():\n",
    "    if (tag1, tag2) in tag_bigrams:\n",
    "        continue\n",
    "    if tag1 not in states or tag2 not in states:\n",
    "        continue\n",
    "    denominator = len(train_tags)\n",
    "    if tag1 in tag_unigrams:\n",
    "        denominator += tag_unigrams[tag1]\n",
    "    model.add_transition(states[tag1], states[tag2], 1/denominator)\n",
    "\n",
    "    \n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 96.19%\n",
      "Testing accuracy: 73.36%\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "\n",
    "training_acc = my_accuracy(X_train, y_train, model, train_words)\n",
    "print(\"Training accuracy: {:.2f}%\".format(100 * training_acc))\n",
    "\n",
    "testing_acc = my_accuracy(X_test, y_test, model, train_words)\n",
    "print(\"Testing accuracy: {:.2f}%\".format(100 * testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: improve accuracy for portuguese using better training dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
